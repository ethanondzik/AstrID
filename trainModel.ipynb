{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c6a31-0548-4f9f-b1ca-2652f2caadde",
   "metadata": {},
   "source": [
    "# **AstrID:**  *model training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c9b41-da1a-431e-8133-f228618ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.initializers import he_uniform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "\n",
    "# Import custom functions to extract Image arrays and Pixel Mask arrays from our created fits files dataset\n",
    "from dataGathering import extractImageArray, extractPixelMaskArray, extract_star_catalog\n",
    "from dataGathering import getStarData, getImagePlot, getPixelMaskPlot\n",
    "from dataGathering import displayRawImage, displayRawPixelMask, displayImagePlot, displayPixelMaskPlot, displayPixelMaskOverlayPlot\n",
    "\n",
    "# Import custom functions to preprocess Image and Pixel Mask arrays\n",
    "from imageProcessing import normalizeImages, stackImages, stackMasks, preprocessImage\n",
    "\n",
    "# Import custom logging function\n",
    "from log import write_to_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8bde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU is available\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    K.clear_session()\n",
    "    tf.config.experimental.reset_memory_stats('GPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d039f4eb",
   "metadata": {},
   "source": [
    "### UnSu Unet Object Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a193a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder path\n",
    "    #32 kernels\n",
    "    #3x3 kernel size\n",
    "    #padding = same considers edges in the input\n",
    "# Example function to create a U-Net model\n",
    "def unet_model(input_shape, filters, kernel_size, activation, padding, initializer):\n",
    "    inputs = layers.Input(input_shape)\n",
    "\n",
    "    # Encoder path\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(inputs)\n",
    "    c1 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p1)\n",
    "    c2 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p2)\n",
    "    c3 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p3)\n",
    "    c4 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(p4)\n",
    "    c5 = layers.Conv2D(filters[4], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c5)\n",
    "\n",
    "    # Decoder path\n",
    "    u6 = layers.Conv2DTranspose(filters[3], (2, 2), strides=(2, 2), padding=padding)(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u6)\n",
    "    c6 = layers.Conv2D(filters[3], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(filters[2], (2, 2), strides=(2, 2), padding=padding)(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u7)\n",
    "    c7 = layers.Conv2D(filters[2], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(filters[1], (2, 2), strides=(2, 2), padding=padding)(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u8)\n",
    "    c8 = layers.Conv2D(filters[1], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(filters[0], (2, 2), strides=(2, 2), padding=padding)(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(u9)\n",
    "    c9 = layers.Conv2D(filters[0], kernel_size, activation=activation, padding=padding, kernel_initializer=initializer)(c9)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
    "\n",
    "    model = models.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b75e80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getStarData('II/246', 250, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create images and masks arrays lists\n",
    "images = []\n",
    "masks = []\n",
    "\n",
    "# Create df to store the star data inside each fits file\n",
    "star_data = []\n",
    "\n",
    "# Create a list of all the fits files in the dataset folder\n",
    "fits_files = os.listdir('data/fits/')\n",
    "\n",
    "# For all the fits files in the dataset folder specified in file_path, extract the image and mask arrays to the respective lists\n",
    "file_path = 'data/fits/'\n",
    "# for file in os.listdir(file_path):\n",
    "for file in os.listdir(file_path):\n",
    "    if file.endswith('.png'):\n",
    "        os.remove(file_path + file)\n",
    "    if file.startswith('data') and file.endswith('.fits'):\n",
    "        images.append(extractImageArray(file_path + file))\n",
    "        masks.append(extractPixelMaskArray(file_path + file))\n",
    "        star_data.append(extract_star_catalog(file_path + file))\n",
    "\n",
    "        print(file + ' added to dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803fd51f",
   "metadata": {},
   "source": [
    "# Prepare the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c650532b",
   "metadata": {},
   "source": [
    "### Convert to 3-Channel Images\n",
    "\n",
    "The images we have are 512 x 512 pixels, but our model requires them to be in the shape `(512, 512, 3)`, similar to standard RGB images. To achieve this, we stack the single-channel images along the last axis three times, converting them into 3-channel images. This transformation is necessary because the model typically expects 3-channel input images.\n",
    "\n",
    "For the masks, the model expects them to be in the shape `(512, 512, 1)`. Therefore, we expand the masks along the last axis to add a new dimension, ensuring they have the correct shape.\n",
    "\n",
    "Additionally, both the images and masks need to be converted to NumPy arrays, as this is the desired format for the training model. Below, we perform these conversions to ensure the data is in the correct format for training.\n",
    "\n",
    "Notice when displaying the shape of the `train_images` list below we see it is an array of 250 images of the shape mentioned above, giving us a shape `(250, 512, 512, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_images = stackImages(images)\n",
    "stacked_masks = stackMasks(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9801c4f0",
   "metadata": {},
   "source": [
    "### Normalize the Images\n",
    "\n",
    "To standardize the pixel values in our images, we need to normalize them to a common range.\n",
    "We will use min-max normalization to scale the pixel values to a range between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce79b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized_images = normalizeImages(stacked_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f855b",
   "metadata": {},
   "source": [
    "###  Prepare the model for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beae7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "\n",
    "# Define the base exponent - Change this number to increase or decrease the number of filters by a power of 2\n",
    "base_exponent = 5\n",
    "#############################################\n",
    "# base_exponent = 5, so filters result in [32, 64, 128, 256, 512], this was the original value\n",
    "# base_exponent = 6, so filters result in [64, 128, 256, 512, 1024]\n",
    "# base_exponent = 7, so filters result in [128, 256, 512, 1024, 2048]\n",
    "# base_exponent = 8, so filters result in [256, 512, 1024, 2048, 4096]\n",
    "# base_exponent = 9, so filters result in [512, 1024, 2048, 4096, 8192]\n",
    "# base_exponent = 10, so filters result in [1024, 2048, 4096, 8192, 16384]\n",
    "\n",
    "# Generate the filters based on powers of 2\n",
    "filters = [2 ** (base_exponent + i) for i in range(5)]\n",
    "#############################################\n",
    "# This list comprehension generates the filter sizes by raising 2 to the powers starting from the base exponent and increasing by 1 for each subsequent filter.\n",
    "#############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define hyperparameters\n",
    "hyperparameters = {\n",
    "    'input_shape': (512, 512, 3),\n",
    "    'filters': filters,\n",
    "    # 'filters': [32, 64, 128, 256, 512],\n",
    "    'kernel_size': (3, 3),\n",
    "    'activation': 'relu',\n",
    "    'padding': 'same',\n",
    "    # 'initializer': he_uniform(),\n",
    "    'initializer': he_uniform,\n",
    "    'optimizer': 'adam',\n",
    "    # 'loss': weightedBinaryCrossEntropy,\n",
    "    'loss': 'binary_crossentropy',\n",
    "    'weights' : {0 : 1.0, 1 : 5.0},\n",
    "    'metrics': ['accuracy'],\n",
    "    'epochs': 100,\n",
    "    'batch_size': 4,\n",
    "    'early_stopping_patience': 10,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 0,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Create and compile the model using hyperparameters\n",
    "model = unet_model(\n",
    "    input_shape=hyperparameters['input_shape'],\n",
    "    filters=hyperparameters['filters'],\n",
    "    kernel_size=hyperparameters['kernel_size'],\n",
    "    activation=hyperparameters['activation'],\n",
    "    padding=hyperparameters['padding'],\n",
    "    initializer=hyperparameters['initializer']\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=hyperparameters['optimizer'],\n",
    "    loss=hyperparameters['loss'],\n",
    "    metrics=hyperparameters['metrics']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932a853",
   "metadata": {},
   "source": [
    "**Create Generators for Training and Validation**:\n",
    "   - Use the `flow` method to create generators for training and validation images and masks.\n",
    "   - Ensure that the `seed` parameter is the same for both image and mask generators to maintain alignment.\n",
    "\n",
    " **Custom Generator**:\n",
    "   - Define a custom generator function `custom_generator` that yields a tuple (images, masks, sample_weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af76ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(stacked_images, stacked_masks, test_size=hyperparameters['test_size'], random_state=hyperparameters['random_state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ff170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of training and validation sets\n",
    "print('Training images: ', train_images.shape)\n",
    "print('Training masks: ', train_masks.shape)\n",
    "print('Validation images: ', val_images.shape)\n",
    "print('Validation masks: ', val_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7f8de9",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851767c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Implement Early stopping to cut useless epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # or another metric like 'val_accuracy'\n",
    "    patience=hyperparameters['early_stopping_patience'],         # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True  # Restores the model to the best state after stopping\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_images, train_masks, \n",
    "    validation_data=(val_images, val_masks), \n",
    "    epochs=hyperparameters['epochs'], \n",
    "    batch_size=hyperparameters['batch_size'],\n",
    "    callbacks=[early_stopping],\n",
    "    class_weight=hyperparameters['weights']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "saved_models_path = 'models/saved_models/'\n",
    "training_size = str(len(train_images))\n",
    "saved_model_name = datetime.datetime.now().strftime(\"%Y_%m_%d-%H%M_\") + training_size + '_unet_model.keras'\n",
    "model.save(saved_models_path + saved_model_name)\n",
    "\n",
    "#log model parameters, time, and user\n",
    "write_to_log(history, hyperparameters, saved_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9056adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training loss and validation loss\n",
    "print('Training loss: ', history.history['loss'][-1])\n",
    "print('Validation loss: ', history.history['val_loss'][-1])\n",
    "\n",
    "# Show training accuracy and validation accuracy\n",
    "print('Training accuracy: ', history.history['accuracy'][-1])\n",
    "print('Validation accuracy: ', history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431a9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(24, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('results/figures/' + saved_model_name.removesuffix('unet_model.keras') + '_training_validation.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322ff4ef-a8b3-4b9f-af7b-cdf0c5437e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a larger figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(history.history['loss'], color='blue', linestyle='-', linewidth=2, label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], color='orange', linestyle='--', linewidth=2, label='Validation Loss')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xlabel('Epochs', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend to differentiate between training and validation loss\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "# Set limits for better visualization\n",
    "plt.xlim(0, len(history.history['loss']) - 1)  # From epoch 0 to the last epoch\n",
    "plt.ylim(min(history.history['loss']) * 0.95, max(history.history['val_loss']) * 1.05)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig('results/figures/' + saved_model_name.removesuffix('unet_model.keras') + '_training_validation_loss.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
